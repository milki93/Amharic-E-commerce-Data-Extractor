{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1750980214631,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "BCNJoOAxY7Ng",
    "outputId": "674f6cd1-6beb-4f48-b90c-e176ba674bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Amharic-E-commerce-Data-Extractor' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/milki93/Amharic-E-commerce-Data-Extractor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1750980216258,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "qmj2WwXLY_Jl",
    "outputId": "3dab7164-91ab-44e5-b847-7d920bd73b41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Amharic-E-commerce-Data-Extractor/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /content/Amharic-E-commerce-Data-Extractor/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFvHQClCxwUh"
   },
   "source": [
    "### Load Dataset and Parse CoNLL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 23156,
     "status": "ok",
     "timestamp": 1750980240154,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "Jt8oD9EnZLdM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load CoNLL Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from collections import defaultdict\n",
    "\n",
    "conll_file_path = \"/content/Amharic-E-commerce-Data-Extractor/data/labeled_data.conll\"\n",
    "unique_labels = ['O', 'B-Product', 'I-Product', 'B-LOC', 'I-LOC', 'B-PRICE', 'I-PRICE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "31a0518c13b24bc78c276753bd875e37",
      "37a36fc47b20402daac07aa18a32ae30",
      "cbcc97b0a0fd4a94a054fd727b15ec9a",
      "d183dedd529a40e1818cbff5dd5be9d5",
      "4d5fe3addf234160a47100e3ba09808e",
      "d9ff3e3ad0194decab80b18c9bd21ed5",
      "6ea1ae54586c4ee6b0bb7ef206a2daab",
      "7c88521dc2c940f084ea66dc74e6a9f3",
      "1b3af80e09834243a350b2930fb6a422",
      "bf47af49a47c448092f675e97333e9ed",
      "fddb587c0b66491290d53c02e9fd27bc"
     ]
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1750980240320,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "jXYWZ95hZOCL",
    "outputId": "6ae71f6f-14bf-479b-89df-e5a8e6d2eb74"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a0518c13b24bc78c276753bd875e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40\n",
      "Validation size: 10\n"
     ]
    }
   ],
   "source": [
    "def parse_conll_file(file_path):\n",
    "    data = []\n",
    "    tokens, ner_tags = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:\n",
    "                    token, label = parts\n",
    "                    tokens.append(token)\n",
    "                    ner_tags.append(label)\n",
    "                else:\n",
    "                    if tokens:\n",
    "                        data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "                    tokens, ner_tags = [], []\n",
    "            else:\n",
    "                if tokens:\n",
    "                    data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "                tokens, ner_tags = [], []\n",
    "        if tokens:\n",
    "            data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "    return data\n",
    "\n",
    "if os.path.exists(conll_file_path):\n",
    "    loaded_data = parse_conll_file(conll_file_path)\n",
    "    ner_dataset = Dataset.from_list(loaded_data)\n",
    "    ner_dataset = ner_dataset.filter(lambda example: len(example['tokens']) > 0)\n",
    "    split_dataset = ner_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    tokenized_datasets = DatasetDict({\"train\": split_dataset[\"train\"], \"validation\": split_dataset[\"test\"]})\n",
    "    print(f\"Train size: {len(tokenized_datasets['train'])}\")\n",
    "    print(f\"Validation size: {len(tokenized_datasets['validation'])}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"CoNLL file not found at: {conll_file_path}. Please ensure it exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvN-Mdezxpsw"
   },
   "source": [
    "### Labels and Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1750980240349,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "eAs6J4UCZzxM"
   },
   "outputs": [],
   "source": [
    "unique_ner_tags = set()\n",
    "for tags in ner_dataset['ner_tags']:\n",
    "    for tag in tags:\n",
    "        unique_ner_tags.add(tag)\n",
    "\n",
    "label_list = sorted(list(unique_ner_tags))\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list.insert(0, 'O')\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\" # ADDED THIS LINE to ensure consistent sequence lengths\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1750980240356,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "DhDtcWpbaw9S"
   },
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lNyoVjExknE"
   },
   "source": [
    "### Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1750980240620,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "eOOgmIkQaf1f",
    "outputId": "8e25d083-cef7-46f0-e8e2-8eff95d296c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-7-124466261.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1750980240623,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "JS9BQW2OasZg"
   },
   "outputs": [],
   "source": [
    "model_checkpoints = [\n",
    "    \"Davlan/xlm-roberta-base-ner-hrl\",\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    \"bert-base-multilingual-cased\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyBdbvlnxaLU"
   },
   "source": [
    "### Model Training for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "05f228879c1940e59efb3a9bed8852a2",
      "787ac641ffd34125a40e87dac29286fc",
      "9bce2e87cbe9480d9966d745bdeb5fdf",
      "c7edbd66f9dc43748c902d560e31493e",
      "f22701e735db41d0a9e540c55f3a16cf",
      "0aea46fad9c84dbba241616b0aac6cd4",
      "9359d3e9ffbe43acaa9fecac9981a85d",
      "be1b13dc308b42a8834c7038bd866f3d",
      "eab62945689c4ddba34f9b84bc43dc32",
      "d99ed26eaa104d8a85f66af5747ebb2d",
      "890822c5c5614b8ebdb1abc05977c15d",
      "67e12e8fb10943148c1deed9814720e8",
      "a822e5f7cdec4f3ca5d5fc53ca5c86f4",
      "d515537068c7488dababb71673707515",
      "df5704fadc1149e783b086f493ad1316",
      "78bbd245c5c944b1b9c7ad8201cfed45",
      "367a5bfb0b2442a78662ec507124a53c",
      "7b552c0fba7545069f56731475a862a1",
      "118f9296e8744e78af1f22cb584d923c",
      "37a8f456cf194d2ea826c1c9b0a209c8",
      "d357bced736b41369eb6b296c8eb3ef1",
      "a6e12aeb2356462589805cb4413d372c",
      "8c2e229bcbf74f35ab3296424d8f79c3",
      "27d525697eb7453eb6136479790874d4",
      "81654161772641f7bd7b2d4cbe5fb3a7",
      "2886ccd3bddf47bd8e1f4d2456ed76d3",
      "093d39338a9f4eba9c4742151c8fb069",
      "5ba9eff3461247bfa57da9a5392e4712",
      "060da8f52be74e05b908f70fe69ea326",
      "aab17e8e1b72451d9d370ebc2f35d7ec",
      "94014ed4d4e04bae903f4d389fecc100",
      "6d1e80ee6c99496eb494207f2c460619",
      "78d6b60f37b84ea5b2442ced0bd9cdd8",
      "e6083bbb50694ea6a48d16dfd6c89645",
      "7a52fc76b93948618356c0ec814d8a3b",
      "d76a2f7abcac40b185f06cb53e1263b6",
      "2e537c544e3b46c18c0600ed1ac0d85e",
      "59b5e9f51c8e4998bce49027192e81ec",
      "561f10a430214c539b76383e412a19e7",
      "f56dead4b5bb4523981d7fb40edc6ffd",
      "576217a8cf9c4f35868ef5f3892d4d4c",
      "3bc9081f486948649f5ead111c145549",
      "a8ab325406fe48b28d52ad94b1ad01df",
      "cb402106f4df452b8b005e278aebdc35",
      "819afe30a20540c289049e2f423db663",
      "a1063f4f5e3c4f45a97632a8cd94423c",
      "d0da9cc45a11459590aea71cf4f4b541",
      "ab90e9dce0954802af8399cfd70bde91",
      "e3cc5221adf34359820f277859f002d6",
      "480496fa58654e8481470e8552ebc3c0",
      "77f81fa6ab714fefb270cca2a11d2065",
      "be70993b2d43447f8f47e0b4d733c3e2",
      "9c3eeda01b98484eb0f5a19e0769a2dc",
      "1ea473d0593a40e4916c127da7ff070a",
      "23e0878954bf4409a662fa57fedaf03a",
      "9fed6a1ec9c54bad8cb5dd5c9ce32124",
      "cd666685cc19455e9775a3ab80466049",
      "6b70403393224ea8bdef5a6d4e83128e",
      "fa28c94bc4c045a9ab4d0c31677402e7",
      "164f35aed77348ab8b68e710010f70de",
      "3001752854bb4b9aa1b311d9b0a9fa8a",
      "f19fbddc1f68425da1f1540c513d19f8",
      "61befea6b2954ea888d7b748b4815b46",
      "5d1e7797497d4d9b8ff64569d06f8c5a",
      "03aed13827394f7dbb167dcba93c7673",
      "c5bb7dc7779f49599ede6754c3f3b6bf"
     ]
    },
    "executionInfo": {
     "elapsed": 2438225,
     "status": "ok",
     "timestamp": 1750983583049,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "o7bywLyZbLWt",
    "outputId": "ecb2c317-0124-4d3f-8a5b-d7692997bf04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Fine-tuning and Evaluation for: Davlan/xlm-roberta-base-ner-hrl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/xlm-roberta-base-ner-hrl and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([7, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer and model for Davlan/xlm-roberta-base-ner-hrl\n",
      "Tokenizing dataset for Davlan/xlm-roberta-base-ner-hrl...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f228879c1940e59efb3a9bed8852a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e12e8fb10943148c1deed9814720e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenization complete.\n",
      "Training Davlan/xlm-roberta-base-ner-hrl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-14-106828083.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 16:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.828600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for Davlan/xlm-roberta-base-ner-hrl completed in 16.73 minutes.\n",
      "Evaluating Davlan/xlm-roberta-base-ner-hrl...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for Davlan/xlm-roberta-base-ner-hrl: {'eval_loss': 0.9116111993789673, 'eval_precision': 0.1, 'eval_recall': 0.0392156862745098, 'eval_f1': 0.05633802816901408, 'eval_accuracy': 0.703971119133574, 'eval_runtime': 18.127, 'eval_samples_per_second': 0.552, 'eval_steps_per_second': 0.165, 'epoch': 3.0}\n",
      "Best model for Davlan/xlm-roberta-base-ner-hrl saved to ./final_models/davlan_xlm_roberta_base_ner_hrl\n",
      "\n",
      " Starting Fine-tuning and Evaluation for: distilbert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer and model for distilbert-base-multilingual-cased\n",
      "Tokenizing dataset for distilbert-base-multilingual-cased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2e229bcbf74f35ab3296424d8f79c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6083bbb50694ea6a48d16dfd6c89645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenization complete.\n",
      "Training distilbert-base-multilingual-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-14-106828083.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 07:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.175700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.026700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for distilbert-base-multilingual-cased completed in 7.39 minutes.\n",
      "Evaluating distilbert-base-multilingual-cased...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for distilbert-base-multilingual-cased: {'eval_loss': 1.2609272003173828, 'eval_precision': 0.09090909090909091, 'eval_recall': 0.0392156862745098, 'eval_f1': 0.05479452054794521, 'eval_accuracy': 0.5324909747292419, 'eval_runtime': 9.0887, 'eval_samples_per_second': 1.1, 'eval_steps_per_second': 0.33, 'epoch': 3.0}\n",
      "Best model for distilbert-base-multilingual-cased saved to ./final_models/distilbert_base_multilingual_cased\n",
      "\n",
      " Starting Fine-tuning and Evaluation for: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded tokenizer and model for bert-base-multilingual-cased\n",
      "Tokenizing dataset for bert-base-multilingual-cased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819afe30a20540c289049e2f423db663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fed6a1ec9c54bad8cb5dd5c9ce32124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenization complete.\n",
      "Training bert-base-multilingual-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-14-106828083.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 13:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for bert-base-multilingual-cased completed in 13.90 minutes.\n",
      "Evaluating bert-base-multilingual-cased...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results for bert-base-multilingual-cased: {'eval_loss': 0.9591091871261597, 'eval_precision': 0.375, 'eval_recall': 0.17647058823529413, 'eval_f1': 0.24, 'eval_accuracy': 0.7021660649819494, 'eval_runtime': 16.6156, 'eval_samples_per_second': 0.602, 'eval_steps_per_second': 0.181, 'epoch': 3.0}\n",
      "Best model for bert-base-multilingual-cased saved to ./final_models/bert_base_multilingual_cased\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AutoConfig\n",
    "\n",
    "comparison_results = {}\n",
    "for checkpoint in model_checkpoints:\n",
    "    print(f\"\\n Starting Fine-tuning and Evaluation for: {checkpoint}\")\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "        config = AutoConfig.from_pretrained(checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(checkpoint, config=config, ignore_mismatched_sizes=True)\n",
    "        print(f\"Successfully loaded tokenizer and model for {checkpoint}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer for {checkpoint}: {e}\")\n",
    "        print(\"Skipping this model and moving to the next one.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Tokenizing dataset for {checkpoint}...\")\n",
    "    current_tokenized_datasets = tokenized_datasets.map(\n",
    "        lambda examples: tokenize_and_align_labels(examples, tokenizer),\n",
    "        batched=True,\n",
    "        load_from_cache_file=False\n",
    "    )\n",
    "    print(\"Dataset tokenization complete.\")\n",
    "\n",
    "    output_dir_name = checkpoint.replace('/', '_').replace('-', '_').lower()\n",
    "    output_dir = f\"./results/{output_dir_name}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=current_tokenized_datasets[\"train\"],\n",
    "        eval_dataset=current_tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(f\"Training {checkpoint}...\")\n",
    "    train_start_time = pd.Timestamp.now()\n",
    "    trainer.train()\n",
    "    train_end_time = pd.Timestamp.now()\n",
    "    training_duration = (train_end_time - train_start_time).total_seconds() / 60\n",
    "    print(f\"Training for {checkpoint} completed in {training_duration:.2f} minutes.\")\n",
    "\n",
    "    print(f\"Evaluating {checkpoint}...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation Results for {checkpoint}: {eval_results}\")\n",
    "\n",
    "    comparison_results[checkpoint] = {\n",
    "        \"F1-score\": eval_results.get(\"eval_f1\", 0.0),\n",
    "        \"Precision\": eval_results.get(\"eval_precision\", 0.0),\n",
    "        \"Recall\": eval_results.get(\"eval_recall\", 0.0),\n",
    "        \"Accuracy\": eval_results.get(\"eval_accuracy\", 0.0),\n",
    "        \"Training Duration (min)\": training_duration,\n",
    "        \"Eval Runtime (s)\": eval_results.get(\"eval_runtime\", 0.0)\n",
    "    }\n",
    "\n",
    "    final_model_save_path = f\"./final_models/{output_dir_name}\"\n",
    "    trainer.save_model(final_model_save_path)\n",
    "    print(f\"Best model for {checkpoint} saved to {final_model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxSAVAHJxQns"
   },
   "source": [
    "### Final Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1750983826103,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "vNcJ1A2wcwy5",
    "outputId": "77a2cac7-2b38-44c0-ea88-9021e4ce783e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MODEL COMPARISON\n",
      "|                                    |   F1-score |   Precision |    Recall |   Accuracy |   Training Duration (min) |   Eval Runtime (s) |\n",
      "|:-----------------------------------|-----------:|------------:|----------:|-----------:|--------------------------:|-------------------:|\n",
      "| bert-base-multilingual-cased       |  0.24      |   0.375     | 0.176471  |   0.702166 |                  13.9046  |            16.6156 |\n",
      "| Davlan/xlm-roberta-base-ner-hrl    |  0.056338  |   0.1       | 0.0392157 |   0.703971 |                  16.7278  |            18.127  |\n",
      "| distilbert-base-multilingual-cased |  0.0547945 |   0.0909091 | 0.0392157 |   0.532491 |                   7.38947 |             9.0887 |\n",
      "\n",
      " Best Performing Model (by F1-score)\n",
      "Recommended Model: bert-base-multilingual-cased\n",
      "Details:\n",
      "F1-score                    0.240000\n",
      "Precision                   0.375000\n",
      "Recall                      0.176471\n",
      "Accuracy                    0.702166\n",
      "Training Duration (min)    13.904617\n",
      "Eval Runtime (s)           16.615600\n",
      " Highest F1-score indicates the best balance of precision and recall.\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL MODEL COMPARISON\")\n",
    "results_df = pd.DataFrame.from_dict(comparison_results, orient='index')\n",
    "results_df_sorted = results_df.sort_values(by='F1-score', ascending=False)\n",
    "print(results_df_sorted.to_markdown())\n",
    "print(\"\\n Best Performing Model (by F1-score)\")\n",
    "best_model_name = results_df_sorted.index[0]\n",
    "print(f\"Recommended Model: {best_model_name}\")\n",
    "print(f\"Details:\\n{results_df_sorted.iloc[0].to_string()}\")\n",
    "print(\" Highest F1-score indicates the best balance of precision and recall.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2198,
     "status": "ok",
     "timestamp": 1750984194140,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "RINTccPUwZ15",
    "outputId": "d03d9a57-fceb-4307-a47a-2eef713ac553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1750984224876,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "0rPP4Ohbym4y",
    "outputId": "4c48daca-36e4-43d8-8526-c241d19b7e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Amharic-E-commerce-Data-Extractor\n"
     ]
    }
   ],
   "source": [
    "%cd /content/Amharic-E-commerce-Data-Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1750984291067,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "d5-bog9Dyu7_",
    "outputId": "1288df9a-1e1d-422a-bfe2-ef0aedfe051e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating public/private ed25519 key pair.\n",
      "Created directory '/root/.ssh'.\r\n",
      "Your identification has been saved in /root/.ssh/id_ed25519\n",
      "Your public key has been saved in /root/.ssh/id_ed25519.pub\n",
      "The key fingerprint is:\n",
      "SHA256:vA8jlTsAAnz0UkTEGyLeFdk1oJ1F6A7Em41dVRw71lk milkidida131@gmail.com\n",
      "The key's randomart image is:\n",
      "+--[ED25519 256]--+\n",
      "|o ..**+.==..oo. E|\n",
      "| + ooBooo..  .o o|\n",
      "|. =.=oXo.    + o |\n",
      "| . o.B = .  . .  |\n",
      "|      + S        |\n",
      "|       + o       |\n",
      "|      . *        |\n",
      "|       . =       |\n",
      "|          .      |\n",
      "+----[SHA256]-----+\n"
     ]
    }
   ],
   "source": [
    "!ssh-keygen -t ed25519 -C \"milkidida131@gmail.com\" -f ~/.ssh/id_ed25519 -N \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1750984439065,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "spRvfNWqyyT0"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ~/.ssh\n",
    "# !eval \"$(ssh-agent -s)\" && ssh-add ~/.ssh/id_ed25519"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1750984445701,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "M328cDPUzCEM"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1750984450764,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "eUSxX7PDzE7T"
   },
   "outputs": [],
   "source": [
    "# !cat ~/.ssh/id_ed25519.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1750984460952,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "I1XJg_YJzJSL"
   },
   "outputs": [],
   "source": [
    "# !ssh -T git@github.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1750984499863,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "9aK8AGPVznb9"
   },
   "outputs": [],
   "source": [
    "!git remote set-url origin git@github.com:milki93/Amharic-E-commerce-Data-Extractor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1750984515712,
     "user": {
      "displayName": "Milki Dida",
      "userId": "07464085856863573424"
     },
     "user_tz": -180
    },
    "id": "vVrC2rrPzuTn",
    "outputId": "dfc277e7-fee7-49b8-9257-001629f47137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'task-4'\n"
     ]
    }
   ],
   "source": [
    "!git checkout -b task-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZthVBu7Iz13R"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/Colab Notebooks/NER_model.ipynb\" /content/Amharic-E-commerce-Data-Extractor/notebooks"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0b47OLrw0HCfcrA9ScGPh",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
